{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3040e8d",
   "metadata": {},
   "source": [
    "Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e318d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.31.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f4dea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you knowÂ ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "header_texts = [header.get_text() for header in headers]\n",
    "df = pd.DataFrame({'Headers': header_texts})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63730fb5",
   "metadata": {},
   "source": [
    "Write a python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d168cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Data Received\n"
     ]
    }
   ],
   "source": [
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    presi_data = []\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    for row in soup.select('table tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        name, term_of_office = columns[0].text.strip(), columns[1].text.strip()\n",
    "        presi_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "    df1 = pd.DataFrame(presi_data)\n",
    "\n",
    "    print(df1)\n",
    "else:\n",
    "    print(\"No Data Received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55c0b9",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data framei) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d94d0abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    headlines_data = []\n",
    "    headlines_container = soup.find('div', class_='Card-titleContainer')\n",
    "    if headlines_container:\n",
    "        headlines = headlines_container.find_all('h3', class_='Card-titleLink')\n",
    "        for headline in headlines:\n",
    "            title = headline.text.strip()\n",
    "\n",
    "            time_container = headline.find_previous('time')\n",
    "            time = time_container.text.strip() if time_container else None\n",
    "\n",
    "            link = headline.find('a')['href'] if headline.find('a') else None\n",
    "\n",
    "            headlines_data.append({'Headline': title, 'Time': time, 'News Link': link})\n",
    "\n",
    "        df_headlines = pd.DataFrame(headlines_data)\n",
    "        print(\"News Details:\")\n",
    "        print(df_headlines)\n",
    "    else:\n",
    "        print(\"Unable to find news headlines data.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f274ade",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.inand make data framei) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "127306c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    restaurant_data = []\n",
    "    # Modify the selectors based on the actual HTML structure of the website\n",
    "    # Below are just placeholders\n",
    "    restaurants = soup.find_all('div', class_='restaurant-container')\n",
    "    for restaurant in restaurants:\n",
    "        name = restaurant.find('h2', class_='restaurant-name').text.strip()\n",
    "        cuisine = restaurant.find('span', class_='cuisine').text.strip()\n",
    "        location = restaurant.find('span', class_='location').text.strip()\n",
    "        ratings = restaurant.find('span', class_='ratings').text.strip()\n",
    "        image_url = restaurant.find('img')['src']\n",
    "\n",
    "        restaurant_data.append({\n",
    "            'Restaurant Name': name,\n",
    "            'Cuisine': cuisine,\n",
    "            'Location': location,\n",
    "            'Ratings': ratings,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "\n",
    "    df_restaurants = pd.DataFrame(restaurant_data)\n",
    "    print(\"Restaurant Details:\")\n",
    "    print(df_restaurants)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
